# Implementation Plan: Production Training Script

## Problem Statement

We need to build the production training script based on the requirements in `the-plan.md` Part C & D, while:
- Documenting what's actually implemented in `the-reality.md`
- Making hyperparameters configurable via `config.yaml`
- Starting with a smaller model to validate the full pipeline
- Reusing validated code from `validation/`

## Context

**What We Have:**
- ✅ Validation code in `validation/` (reviewed, debugged, proven on 100-sample overfitting test)
- ✅ WebDataset creation script (`scripts/create_webdataset_shards.py`)
- ✅ >3k photos with captions + T5 + VAE + DINOv3 embeddings
- ✅ Working architecture: NanoDiT (12 layers, 384 hidden, dual conditioning)

**What We Need:**
- Production training script that can scale from small to full size
- Configurable hyperparameters via YAML
- Documentation of actual implementation vs theoretical plan
- Aspect ratio bucketing (already in webdataset script)
- Smart augmentation (horizontal flip with caption swapping)

## Approach

### Phase 1: Copy & Adapt (Foundation)
**Goal:** Get production training running with minimal changes to proven validation code

**Strategy:**
1. **Copy validation code wholesale** to `production/` folder
   - Keeps validation/ as reference documentation
   - Allows production/ to evolve independently
   - Preserves working baseline for debugging

2. **Create config.yaml** following the-plan.md Part D specs
   - Model architecture (layers, hidden_size, heads, patch_size)
   - Training hyperparameters (lr, warmup, batch_size, grad_accum)
   - Data settings (shard_dir, bucket strategy, augmentation)
   - Sampling/validation (CFG scales, num_steps, checkpointing)

3. **Replace hardcoded values** with config loading
   - Model dimensions
   - Training schedule
   - CFG dropout probabilities
   - Checkpoint intervals

### Phase 2: Scale Up (Architecture)
**Goal:** Support production-scale models while starting small

**Changes needed:**
1. **Make model configurable**
   - Support 12 layers (validation) → 24-28 layers (production)
   - Support 384 hidden (validation) → 768-1024 hidden (production)
   - Support patch_size 2 (high detail) as per plan
   - Keep all other architecture elements (adaLN-Zero, cross-attention, etc.)

2. **Add gradient accumulation**
   - Validation uses batch_size 8, no accumulation
   - Production needs effective_batch_size 256
   - Implement: accumulate_grads=32 → effective_batch=8×32=256

3. **Add advanced features from the-plan.md**
   - Logit-normal timestep sampling (vs uniform)
   - EMA warmup schedule (instead of fixed 0.9999)
   - Gradient checkpointing flag (memory vs speed tradeoff)
   - Zero-init for adaLN gates (already in validation/)

### Phase 3: Data Pipeline (Production Ready)
**Goal:** Handle full dataset with proper bucketing and augmentation

**Changes needed:**
1. **Use WebDataset shards** (already created by script)
   - validation/ reads single tar file
   - production/ reads multiple shards from bucket directories
   - Implement bucket-aware batching (all samples in batch same resolution)

2. **Smart augmentation**
   - Horizontal flip with p=0.5
   - Caption swapping: `\bleft\b ↔ \bright\b` when flipped
   - No color jitter, no random crop (preserves quality)

3. **Multi-bucket support**
   - Read shards from `data/shards/{bucket_name}/shard-*.tar`
   - Sample buckets proportionally to dataset size
   - Ensure batch dimensions are uniform per bucket

### Phase 4: Documentation (Truth vs Plan)
**Goal:** Create `the-reality.md` documenting actual implementation

**Structure:**
```markdown
# The Reality: Production Training Implementation

## What Matches the-plan.md
- List features implemented exactly as planned

## What Differs from the-plan.md
- Document deviations with rationale

## Implementation Status
- What's working
- What's tested
- What's TODO

## Training Recipes
- Command examples
- Expected behavior
- Troubleshooting
```

## Implementation Workplan

### Stage 1: Foundation (Copy & Config)
- [ ] Create `production/` directory structure
- [ ] Copy validation code: model.py, train.py, data.py, sample.py
- [ ] Create `production/config.yaml` with full spec from the-plan.md
- [ ] Implement config loading in all modules
- [ ] Create `production/train_production.py` entry point
- [ ] Test: train small model (12 layers) with config-driven params

### Stage 2: Scaling Infrastructure
- [ ] Add model size parameters to config (depth, hidden_size, patch_size)
- [ ] Implement gradient accumulation in train.py
- [ ] Add logit-normal timestep sampling
- [ ] Add EMA warmup schedule
- [ ] Add gradient checkpointing flag
- [ ] Test: train 24-layer model (still small hidden_size for speed)

### Stage 3: Data Pipeline
- [ ] Modify data.py to read multi-bucket WebDataset shards
- [ ] Implement horizontal flip augmentation
- [ ] Implement caption left/right swapping
- [ ] Add bucket-aware batching logic
- [ ] Test: train on full 3k+ dataset with augmentation

### Stage 4: Production Hardening
- [ ] Add comprehensive logging (loss, grad_norm, lr per step)
- [ ] Add validation callback with configurable frequency
- [ ] Implement monitoring from the-plan.md Part E
  - Velocity field norm tracking
  - Divergence test (text-only vs dino-only CFG)
  - Gradient norm warnings
- [ ] Create checkpoint resume functionality
- [ ] Test: full training run 10k-50k steps

### Stage 5: Documentation
- [ ] Create `the-reality.md` documenting actual implementation
- [ ] Document config.yaml format and options
- [ ] Add production/ README with usage examples
- [ ] Document differences from the-plan.md with rationale
- [ ] Add training recipes (starter configs for different scales)

## Key Design Decisions

### Config Structure (Recommended)
```yaml
# config.yaml
model:
  depth: 12                    # Number of transformer layers
  hidden_size: 384             # Hidden dimension
  num_heads: 6                 # Attention heads
  patch_size: 2                # Spatial downsampling (2 for high detail)
  mlp_ratio: 4.0               # MLP expansion factor
  
training:
  total_steps: 50000
  warmup_steps: 5000
  batch_size: 8
  grad_accumulation_steps: 32  # Effective batch = 8×32 = 256
  
  optimizer:
    type: "AdamW"
    lr: 3.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.03
    eps: 1.0e-8
  
  grad_clip: 1.0
  ema_decay: 0.9999
  ema_warmup_steps: 5000       # Warm EMA from 0 to target
  
  # CFG dropout (mutually exclusive categorical)
  cfg_dropout:
    p_uncond: 0.1              # Drop both
    p_text_only: 0.1           # Drop DINO
    p_dino_only: 0.1           # Drop text
    # Remaining 70% keeps both

  # Timestep sampling
  timestep_sampling: "logit_normal"  # or "uniform"
  logit_normal_loc: 0.0
  logit_normal_scale: 1.0
  
  # Memory optimization
  gradient_checkpointing: false  # Enable if OOM

data:
  shard_base_dir: "data/shards"
  buckets:                     # List of bucket directories to read
    - "1024x1024"
    - "832x1216"
    - "1216x832"
    - "768x1280"
    - "1280x768"
  
  # Augmentation
  horizontal_flip_prob: 0.5
  swap_caption_lr: true        # Swap left/right in caption when flipped
  
  # Loading
  num_workers: 4
  prefetch_factor: 2
  
sampling:
  num_steps: 50                # Euler integration steps
  text_scale: 3.0              # CFG scale for text
  dino_scale: 2.0              # CFG scale for DINO
  
validation:
  interval_steps: 5000         # Run validation every N steps
  num_samples: 100             # Samples to validate on
  output_dir: "validation_outputs"
  
checkpoint:
  save_every: 5000
  keep_last_n: 3               # Keep only last 3 checkpoints
  output_dir: "checkpoints"
```

### Code Organization
```
production/
├── config.yaml              # Main configuration
├── train_production.py      # Entry point (like run_validation.py)
├── model.py                 # NanoDiT (copied from validation/)
├── train.py                 # Trainer class (adapted from validation/)
├── data.py                  # WebDataset loader (adapted)
├── sample.py                # Euler sampler (copied from validation/)
├── config_loader.py         # NEW: YAML config loading
└── utils.py                 # NEW: logit_normal sampling, etc.
```

### Divergence from validation/
- **Config-driven:** All hyperparameters from YAML, not hardcoded
- **Gradient accumulation:** For large effective batch sizes
- **Multi-bucket:** Read from multiple aspect ratio buckets
- **Smart augmentation:** Flip + caption swap
- **Scalable architecture:** Support 12-28 layers, 384-1024 hidden
- **Advanced sampling:** Logit-normal timesteps, EMA warmup

## Risk Mitigation

### Known Challenges
1. **Multi-bucket batching complexity**
   - Risk: Mixing resolutions in same batch → tensor size errors
   - Mitigation: Strict bucket-aware sampler, validate batch dimensions
   
2. **Gradient accumulation bugs**
   - Risk: Optimizer step at wrong intervals, gradient leaks
   - Mitigation: Test with accumulation=1 first, verify loss convergence
   
3. **Config schema drift**
   - Risk: Code expects keys that don't exist in config
   - Mitigation: Use dataclasses/pydantic for config validation
   
4. **OOM on larger models**
   - Risk: 24+ layers won't fit even with batch_size=1
   - Mitigation: Gradient checkpointing flag, test incrementally

### Validation Strategy
- Start with 12-layer model (proven), verify config loading works
- Scale to 24 layers with small hidden_size (test depth scaling)
- Scale to 24 layers with large hidden_size (test full production size)
- Each stage: verify loss convergence, check generated samples

## Success Criteria

### Stage 1 Complete When:
- [ ] production/ runs with config.yaml
- [ ] Generates same quality as validation/ on small model
- [ ] All hyperparameters configurable via YAML

### Stage 2 Complete When:
- [ ] Can train 24-layer model without OOM
- [ ] Gradient accumulation produces same results as large batch
- [ ] EMA warmup shows smooth convergence

### Stage 3 Complete When:
- [ ] Trains on full 3k+ dataset with all buckets
- [ ] Augmentation verified (flipped images have swapped L/R captions)
- [ ] Batches are uniform resolution per bucket

### Stage 4 Complete When:
- [ ] 50k step run completes without crashes
- [ ] Checkpoints can be resumed mid-training
- [ ] Validation runs automatically at configured intervals
- [ ] Monitoring detects anomalies (grad explosion, collapse)

### Stage 5 Complete When:
- [ ] the-reality.md documents actual implementation
- [ ] README has usage examples and troubleshooting
- [ ] Training recipes tested and verified

## Timeline Estimate

**Assuming 2-3 hour work sessions:**
- Stage 1: 1-2 sessions (copy, config, basic integration)
- Stage 2: 2-3 sessions (scaling features, testing)
- Stage 3: 2-3 sessions (data pipeline, augmentation)
- Stage 4: 2-3 sessions (production hardening, long runs)
- Stage 5: 1 session (documentation)

**Total: 8-12 sessions (~20-30 hours)**

This is a "steady progress" estimate assuming no major architecture surprises. Budget 50% more time for debugging edge cases.

## Notes & Recommendations

### Why Copy Instead of Refactor?
- validation/ is **proven and documented** after extensive debugging
- Keeping it intact provides reference when production/ has issues
- Changes can be compared side-by-side for debugging
- Low risk approach: if production/ breaks, validation/ still works

### Why Start Small?
- Validates entire pipeline (config → data → training → sampling) quickly
- Catches integration bugs before spending GPU time on large models
- Proves gradient accumulation math before burning hours on incorrect runs
- Builds confidence in infrastructure before scale-up

### Config vs Hardcoded?
- Config wins: enables experimentation without code changes
- Enables training recipes (small.yaml, medium.yaml, production.yaml)
- Makes the-reality.md more concrete (config shows actual values used)
- Trade-off: More indirection, but worth it for flexibility

### the-reality.md Purpose
- Documents **actual working implementation** vs idealized plan
- Captures rationale for deviations (e.g., "Used uniform sampling because logit-normal was unstable")
- Living document: update as we learn what actually works
- Helps debug: "What did we actually implement?" vs "What did we plan?"

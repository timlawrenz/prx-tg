# Unsloth Gemma3 Integration

## Summary

Successfully integrated Unsloth's optimized Gemma3-27B model for image captioning in `scripts/generate_approved_image_dataset.py`.

## Changes Made

### 1. Dependencies (`scripts/requirements-approved-image-embeddings.txt`)
- Added `unsloth==2026.2.1` and `unsloth_zoo==2026.2.1`
- Pinned `transformers==4.57.6` (Unsloth compatibility)
- Added required dependencies: `datasets==4.3.0`, `trl==0.24.0`, `peft==0.18.1`, etc.
- Added `bitsandbytes` (use custom ROCm build: copy libbitsandbytes_rocm72.so to venv)

### 2. Model Configuration
- Changed `GEMMA_MODEL_ID` from `google/gemma-3-27b-it` to `unsloth/gemma-3-27b-it-bnb-4bit`
- Uses Unsloth's dynamic 4-bit quantization for optimal VRAM usage

### 3. Code Modifications (`scripts/generate_approved_image_dataset.py`)

#### `load_caption_pipeline()` Function
- Replaced `transformers.pipeline()` with `unsloth.FastVisionModel.from_pretrained()`
- Returns dict with `{"model":, "tokenizer":, "device":}` instead of pipeline object
- Enables Unsloth's inference mode automatically
- Sets `UNSLOTH_SKIP_TORCHVISION_CHECK=1` to silence warnings

#### `generate_captions()` Function
- Processes images individually (batch_size=1) for stability
- Uses manual tokenization + model.generate() instead of pipeline API
- Improved output parsing to handle Gemma3 chat template format
- Preserves `ensure_single_paragraph()` call for consistency

## Benefits

1. **1.6x faster** captioning throughput (per Unsloth benchmarks)
2. **60% less VRAM** usage via dynamic 4-bit quantization
3. **Float16 stability** fixes for RTX/ROCm GPUs (critical for 4090)
4. **Same quality** captions with improved performance

## Usage

```bash
# Install dependencies
source .venv/bin/activate
pip install -r scripts/requirements-approved-image-embeddings.txt

# Copy custom bitsandbytes for ROCm (if needed)
cp /home/tim/activity/bitsandbytes/bitsandbytes/libbitsandbytes_rocm72.so \
   .venv/lib/python3.13/site-packages/bitsandbytes/

# Run captioning (same as before)
python scripts/generate_approved_image_dataset.py --caption-only
```

## Testing

Test script created: `test_unsloth_caption.py`
```bash
UNSLOTH_SKIP_TORCHVISION_CHECK=1 python test_unsloth_caption.py
```

## Notes

- First run will download ~20GB model weights to HuggingFace cache
- Model uses 4-bit quantization, fits comfortably in 16GB+ VRAM
- For even lower VRAM, can try `unsloth/gemma-3-12b-it-bnb-4bit` (smaller model)
- All existing script features preserved (resume, Stage 2 processing, etc.)

## Troubleshooting

### ModuleNotFoundError: No module named 'datasets'
```bash
pip install datasets==4.3.0
```

### bitsandbytes ROCm error
Copy the custom-compiled library:
```bash
cp /path/to/libbitsandbytes_rocm72.so .venv/lib/python3.13/site-packages/bitsandbytes/
```

### Torchvision version warning
Set environment variable:
```bash
export UNSLOTH_SKIP_TORCHVISION_CHECK=1
```

## Update: Batch Processing Fix (2026-02-15)

### Issue
Initial implementation processed images one-at-a-time, ignoring `--batch-size` parameter.

### Fix
- Modified `generate_captions()` to properly batch process multiple images
- Added fallback to individual processing if batch mode fails
- Extracted caption cleaning logic to `_clean_caption_output()` helper function

### Behavior
- `--batch-size 1`: Processes images individually (safest, slower)
- `--batch-size 4`: Attempts batch processing of 4 images at once
  - Falls back to individual processing if batch fails
  - Batch processing may be unstable for vision models depending on VRAM

### Testing
```bash
# Test with batch size 4
UNSLOTH_SKIP_TORCHVISION_CHECK=1 python scripts/generate_approved_image_dataset.py \
  --pass dinov3 --batch-size 4 --verbose

# Should see: "verbose: generating captions for batch of 4..."
```

## Update: Startup Warnings Fixed (2026-02-15)

### Issues
Several warnings appeared at startup:
1. `TRANSFORMERS_CACHE` is deprecated (use `HF_HOME`)
2. Torchvision version warning from Unsloth
3. Import order warning (unsloth should be imported before transformers)
4. `torch_dtype` is deprecated (use `dtype`)

### Fixes Applied

#### 1. Import Order Optimization
Moved `import unsloth` to the top of the script (line 18), **before** any transformers imports.
This ensures Unsloth's optimizations are properly applied for maximum performance.

#### 2. Environment Variables
Set environment variables at script startup:
- `UNSLOTH_SKIP_TORCHVISION_CHECK=1` - Silences torchvision version warnings
- `HF_HOME` - Uses modern HuggingFace cache location

#### 3. T5 Loading Update
Changed `torch_dtype=torch.float16` to `dtype=torch.float16` in `load_t5_encoder()`

#### 4. Wrapper Script
Created `scripts/run-dataset-generation.sh` for convenient execution with proper environment:

```bash
# Use the wrapper script (recommended)
./scripts/run-dataset-generation.sh --pass all --batch-size 4

# Or set environment variables manually
export UNSLOTH_SKIP_TORCHVISION_CHECK=1
export HF_HOME=~/.cache/huggingface
python scripts/generate_approved_image_dataset.py --pass all
```

### Expected Output (Clean)
After these fixes, you should see minimal warnings:
```
disk space check: 9802.6 GB available at data/derived
loaded 3490 existing records from data/derived/approved_image_dataset.jsonl
device: cuda
verbose: loading T5 tokenizer...
verbose: loading DINOv3 model...
verbose: loading caption model...
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
loading Unsloth Gemma3 model from unsloth/gemma-3-27b-it-bnb-4bit...
âœ“ Unsloth Gemma3 loaded successfully
```

## Update: Diffusers Version Fix (2026-02-15)

### Issue
Diffusers 0.31+ introduced a bug in `torchao_quantizer.py` where `logger` is not defined:
```
RuntimeError: Failed to import diffusers.models.autoencoders.autoencoder_kl
NameError: name 'logger' is not defined
```

This breaks VAE loading: `--pass vae` would fail immediately.

### Fix
Pinned diffusers to `0.30.3` (last stable version before the bug).

### Updated Requirements
```
diffusers==0.30.3  # Pinned to avoid torchao bug in 0.31+
```

### Verification
```bash
# Test VAE pass (should work now)
python scripts/generate_approved_image_dataset.py --pass vae --limit 10
```

This fix is essential for the full pipeline to work correctly.

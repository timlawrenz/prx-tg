## Context

We now have `data/approved/` containing the approved images (symlinks with correct extensions). We want to generate a training/analysis dataset where each approved image has:
- a vision embedding (DINOv3 via HF model `facebook/dinov3-vitl16-pretrain-lvd1689m`)
- a dense, dry caption (~300 tokens) generated by HF model `google/gemma-3-27b-it` using the provided prompt

The output should be a JSONL file with one record per image.

Constraints:
- Potentially large number of images; this is a long-running batch job.
- Must be restartable (resume without redoing completed work).
- Model inference is expensive; GPU strongly preferred.

## Goals / Non-Goals

**Goals:**
- Iterate images in `data/approved/` and produce `{image_path, embedding, caption}` JSONL records.
- Use DINOv3 embeddings derived from image pixels (not filename/metadata).
- Generate exactly one dense paragraph caption using the provided prompt.
- Provide robust progress reporting and resumability.

**Non-Goals:**
- Changing which images are approved (we consume whatever is present in `data/approved/`).
- Pruning/rewriting `data/approved/` symlinks.
- Building a serving API; this is an offline batch pipeline.

## Decisions

1) **Scripted batch pipeline (Python + Hugging Face)**
- Implement as a Python script under `scripts/` using `torch` + `transformers` (and image loading via `PIL`/`pillow`).
- Rationale: fastest path to run both models locally with GPU support.

2) **Deterministic iteration order + resume semantics**
- Iterate files in a stable order (e.g., sort by filename) to make runs reproducible.
- Write output as JSONL append-only.
- Resume by tracking completed images:
  - simplest: read existing JSONL and build a set of completed `image_path`s (may be memory heavy if very large)
  - preferred: maintain a lightweight sidecar index (e.g., `output.jsonl.idx` with one path per line) or a SQLite checkpoint
- Rationale: avoid redoing expensive inference.

3) **Output format**
- JSONL record per image containing:
  - `image_path` (relative path like `data/approved/<name>.<ext>`)
  - `dinov3_embedding` (vector)
  - `caption` (string)
- Consider optional gzip output (e.g., `output.jsonl.gz`) due to embedding size, but keep the canonical format JSONL.

4) **DINOv3 embedding extraction**
- Use the HF model `facebook/dinov3-vitl16-pretrain-lvd1689m` with a matching image processor.
- Extract a single fixed-length embedding per image (e.g., pooled CLS / model-provided pooled output).
- Normalize embeddings (e.g., L2 normalize) if intended for similarity search.

5) **Caption generation**
- Use `google/gemma-3-27b-it` in chat/instruction mode.
- Enforce the prompt:
  `You are generating a caption for a text-to-image training dataset. Write exactly one dense paragraph in a dry, descriptive tone (no flowery language, no lists). Describe only what is visible in the image; do not guess or invent details. Include (when visible): subject, pose, clothing/accessories, lighting, background, composition/framing, and camera angle.`
- Constrain output length to ~300 tokens via `max_new_tokens` and stop sequences if needed.

6) **Progress reporting**
- Print periodic counters (images processed, images skipped, throughput) and current filename.
- Allow limiting for smoke tests: `--limit N`.

## Risks / Trade-offs

- **[Very large output JSONL] → Mitigation:** support gzip output; consider embedding compression/encoding strategy.
- **[Model availability / hardware constraints] → Mitigation:** allow selecting device (`cuda`/`cpu`) and batch size; document expected runtime.
- **[Non-deterministic text generation] → Mitigation:** fix seed, temperature=0 (or low), and deterministic decoding.
- **[Resume bookkeeping complexity] → Mitigation:** choose a simple, explicit checkpoint format (sidecar index or sqlite).

## Migration Plan

- Run the batch script to generate the JSONL dataset.
- Re-run periodically when `data/approved/` changes; resume behavior should avoid reprocessing prior entries.

## Open Questions

- Where should the canonical output live (e.g., `data/derived/approved-image-embeddings.jsonl`)?
- What embedding representation should we store in JSONL (full float list vs base64-packed float16) to balance size vs simplicity?
- Do we require L2-normalized embeddings, or raw model outputs?
- Should captions be forced to exactly one paragraph via post-processing (e.g., replace newlines)?

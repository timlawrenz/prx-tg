# Production Training Configuration
# Based on the-plan.md Part D specifications

model:
  depth: 12                    # Number of transformer layers (start small, scale to 24-28)
  hidden_size: 384             # Hidden dimension (scale to 768-1024 for production)
  num_heads: 6                 # Attention heads
  patch_size: 2                # Spatial downsampling (2 for high detail as per plan)
  mlp_ratio: 4.0               # MLP expansion factor
  in_channels: 16              # VAE latent channels
  input_size: 64               # Latent spatial size (512px / 8 = 64)

training:
  total_steps: 50000           # Total training steps
  warmup_steps: 5000           # Linear LR warmup (10% of total)
  batch_size: 8                # Per-GPU batch size
  grad_accumulation_steps: 32  # Effective batch = 8×32 = 256
  
  optimizer:
    type: "AdamW"
    lr: 3.0e-4                 # Peak learning rate (from-scratch training)
    min_lr: 1.0e-6             # Minimum LR after cosine decay
    betas: [0.9, 0.95]         # Beta values (0.95 faster than standard 0.999)
    weight_decay: 0.03         # Standard regularization
    eps: 1.0e-8
  
  grad_clip: 1.0               # Gradient clipping norm
  ema_decay: 0.9999            # Target EMA decay
  ema_warmup_steps: 5000       # Warm EMA from 0 to target over N steps
  
  # CFG dropout (mutually exclusive categorical as per fixed validation code)
  cfg_dropout:
    p_uncond: 0.1              # Drop both DINO and text (unconditional)
    p_text_only: 0.1           # Drop DINO (text-only conditioning)
    p_dino_only: 0.1           # Drop text (DINO-only conditioning)
    # Remaining 70% keeps both signals

  # Timestep sampling strategy
  timestep_sampling: "logit_normal"  # "uniform" or "logit_normal"
  logit_normal_loc: 0.0              # Mean of logit-normal distribution
  logit_normal_scale: 1.0            # Std of logit-normal distribution
  
  # Memory optimization
  gradient_checkpointing: false      # Enable if OOM (trades speed for memory)
  
  # Precision
  mixed_precision: true              # Use bfloat16 (required for flow matching)
  precision: "bfloat16"              # "bfloat16" or "float32"

data:
  shard_base_dir: "data/shards/3000"      # Base directory for WebDataset shards
  
  # Aspect ratio buckets to read (each bucket is a subdirectory with tar files)
  buckets:
    - "bucket_1024x1024"
    - "bucket_1216x832"
    - "bucket_1280x768"
    - "bucket_1344x704"
    - "bucket_704x1344"
    - "bucket_768x1280"
    - "bucket_832x1216"
  
  # Augmentation (smart augmentation from the-plan.md)
  horizontal_flip_prob: 0.5          # Flip probability
  swap_caption_lr: true              # Swap left↔right in caption when flipped
  
  # DataLoader settings
  num_workers: 4                     # Parallel workers
  prefetch_factor: 2                 # Batches to prefetch per worker
  pin_memory: true                   # Pin memory for faster GPU transfer

sampling:
  num_steps: 50                      # Euler integration steps
  text_scale: 3.0                    # CFG scale for text conditioning
  dino_scale: 2.0                    # CFG scale for DINO conditioning

validation:
  enabled: true                      # Enable validation runs
  interval_steps: 5000               # Run validation every N steps
  num_samples: 100                   # Number of samples to validate
  output_dir: "validation_outputs"   # Validation output directory
  
  # Validation tests to run
  run_reconstruction: true           # Test reconstruction LPIPS
  run_dino_swap: true                # Test DINO swap pairs
  run_text_manip: true               # Test text manipulation

checkpoint:
  save_every: 5000                   # Save checkpoint every N steps
  keep_last_n: 3                     # Keep only last N checkpoints (0 = keep all)
  output_dir: "checkpoints"          # Checkpoint output directory
  save_optimizer: true               # Save optimizer state (needed for resume)

logging:
  log_every: 50                      # Log metrics every N steps
  log_file: "training.log"           # JSON-lines log file
  
  # Monitoring (from the-plan.md Part E)
  monitor_velocity_norm: true        # Track |v_pred| magnitude
  monitor_grad_norm: true            # Track gradient norm
  velocity_norm_warning: 10.0        # Warn if |v_pred| > this (collapse detection)
  grad_norm_warning: 10.0            # Warn if grad_norm > this after warmup

# Paths (relative to project root)
paths:
  vae_path: "models/vae.safetensors"         # Flux VAE checkpoint
  t5_path: "models/t5xxl_fp16.safetensors"   # T5-XXL encoder (for validation)
